{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix properties\n",
    "### example Matrix\n",
    "\n",
    "\\begin{equation}\n",
    "\\underbrace{\\begin{pmatrix} 1 & 2 & 4 \\\\ 1 & 1 & 2 \\\\ 1 & 1 & 2 \\\\ \\end{pmatrix}}_{\\text{matrix A 3x3}} \\underbrace{\\begin{pmatrix} x \\\\ y \\\\ z \\\\ \\end{pmatrix}}_{\\text{ vector 3x1}} = \\underbrace{\\begin{pmatrix} x' \\\\ y' \\\\ z' \\\\ \\end{pmatrix}}_{\\text{image 3x1}} = \\underbrace{\\begin{pmatrix} 1x+2y+4z \\\\ 1x+1y+2z \\\\ 1x+1y+2z \\\\ \\end{pmatrix}}_{\\text{image 3x1}} = x \\underbrace{\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ \\end{pmatrix} + y \\underbrace{\\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\end{pmatrix} + z \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\\\ \\end{pmatrix}}_{\\text{linear dependent!}}}_{\\text{columnspace}}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating nullspace\n",
    "\n",
    "\\begin{equation}\n",
    "\\underbrace{\\begin{pmatrix} 1 & 2 & 4 \\\\ 1 & 1 & 2 \\\\ 1 & 1 & 2 \\\\ \\end{pmatrix}}_{\\text{Matrix A 3x3}} \\underbrace{\\begin{pmatrix} x \\\\ y \\\\ z \\\\ \\end{pmatrix}}_{\\text{ Vektor 3x1}} = \\underbrace{\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\end{pmatrix}}_{\\text{zero vector}} \\\\\n",
    "\\Leftrightarrow\n",
    "\\underbrace{\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ \\end{pmatrix}}_{\\text{Matrix A 3x3}} \\underbrace{\\begin{pmatrix} x \\\\ y \\\\ z \\\\ \\end{pmatrix}}_{\\text{ Vektor 3x1}} = \\underbrace{\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\end{pmatrix}}_{\\text{zero vector}} \\\\\n",
    "\\Rightarrow x=0 \\quad \\text{and} \\quad y=2z \\\\\n",
    "\\Rightarrow nul(A) = \\alpha \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\\\ \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "### matrix properties\n",
    "\n",
    "<b>kernel (ker) / nullspace (nul):</b><br>\n",
    "set of all vectors wich get maped to the zero vector by the transformation A. <br>\n",
    "\n",
    "<b>corank / defekt :</b><br>\n",
    "corank(A) = dim( ker(A) ) <br>\n",
    "\n",
    "<b>image (im):</b><br>\n",
    "set of all vectors wich get maped to non-zero vectors by the transformation. Equal to columnspace (and rowspace) of the matrix (after removing linear dependent entries). <br>\n",
    "\n",
    "<b>rank (rank):</b><br>\n",
    "rank(A) = dim( im(A) ) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rank-nullity theorem (Rangsatz)\n",
    "\\begin{equation}\n",
    "dim(A) = \\underbrace{dim(ker(A))}_{corank(A)} + \\underbrace{dim(im(A))}_{rank(A)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unitary transformation\n",
    "A unitary transform preserves length:<br>\n",
    "$<x,y> = <Ux,Uy> = <x,U^H Uy>$ <br>\n",
    "From this follows, that the conjugate transpose is equal to the inverse:<br>\n",
    "$U U^H = U^H U = \\mathbb{1}$ ; $U^+=U^H$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eigenvalues and eigenvectors\n",
    "\n",
    "Eigenvectors are vectors, wich only change length by multiplication with A but keep they previous direction.<br> The corresponding eigenvalues are the scale-factors, by wich the vectors change lenght.\n",
    "\n",
    "\\begin{equation}\n",
    "Ax = \\lambda x \\\\\n",
    "\\Leftrightarrow Ax = \\mathbb{1} \\lambda x \\\\\n",
    "\\Leftrightarrow (A-\\mathbb{1} \\lambda)x=0 \\\\\n",
    "\\Rightarrow det(A-\\lambda) = 0\n",
    "\\end{equation}\n",
    "\n",
    "=> $\\lambda_{1,2,...}$ (Eigenvalues)\n",
    "\n",
    "\\begin{equation}\n",
    "(A-\\mathbb{1} \\lambda)x=0\n",
    "\\end{equation}\n",
    "\n",
    "=> $x_{1,2,...}$ (Eigenvectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution to linear system of equations\n",
    "\n",
    "A solution to the system of linear equations $Ax=b$ is given by $x=A^{-1}b$, where $A^{-1}$ can be computed by manipulation of matrix A:\n",
    "\n",
    "\\begin{equation}\n",
    "Ax = \\mathbb{1} b \\qquad \\Leftrightarrow \\qquad \\mathbb{1} x = A^{-1} b\n",
    "\\end{equation}\n",
    "\n",
    "or with the adjugate and determinant of A:\n",
    "\n",
    "\\begin{equation}\n",
    "A^{-1} = \\frac{adj(A)}{det(A)} \\\\\n",
    "\\end{equation}\n",
    "\n",
    "with the adjugate equal to the adjungate transpose of the cofactor matrix $adj(A)=cof(A)^H$ , where every element of the cofactor matrix $c_{i,j}=cof(A)$ at position i,j is caclulated by the minor (sub-determinant) of A at this position $M_{i,j}$ times $(-1)^{i+j}$.<br><br>\n",
    "The solution only exist, if b is in the columspace of A ($b \\in im(A)$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular value decomposition SVD\n",
    "A Matrix X can be decomposed into two unitary matrices U and V and a diagonal matrix $\\Sigma$.\n",
    "\n",
    "\\begin{equation}\n",
    "X = U \\Sigma V^H \\\\\n",
    "\\Leftrightarrow X^H = V \\Sigma U^H\n",
    "\\end{equation}\n",
    "\n",
    "<b>motivation</b><br>\n",
    "The matrix X of size n x m can be composed of m columnvectors $x_i$, wich represent one data instance (<b>sample</b>). One sample has multiple <b>parameters</b> e.g. size, weight and height of one signle person or all pixel values from one picture of a persons face. Usually there are more data parameters than data samples (n>>m).\n",
    "\n",
    "\\begin{equation}\n",
    "\\underbrace{\\begin{pmatrix} | & ... & | \\\\ x_1 & ... & x_m \\\\ | & ... & | \\\\ \\end{pmatrix}}_{n x m} = \\underbrace{\\begin{pmatrix} | & ... & | \\\\ u_1 & ... & u_n \\\\ | & ... & | \\\\ \\end{pmatrix}}_{n x n} \\underbrace{\\begin{pmatrix} \\sigma_1 & 0 & 0 \\\\ 0 & ... & 0 \\\\ 0 & 0 & \\sigma_m \\\\ 0 & 0 & 0 \\\\ \\end{pmatrix}}_{n x m} \\underbrace{\\begin{pmatrix} - & v_1^H & - \\\\ ... & ... & ... \\\\ - & v_m^H & - \\\\ \\end{pmatrix}}_{m x m}\n",
    "\\end{equation}\n",
    "\n",
    "If the matrix X is a square matrix of full rank, the matrix decoposition simplifies (so called matrix diagonalization with $U=V$):\n",
    "\n",
    "\\begin{equation}\n",
    "X = U \\Sigma U^H \\\\\n",
    "\\underbrace{\\begin{pmatrix} | & ... & | \\\\ x_1 & ... & x_m \\\\ | & ... & | \\\\ \\end{pmatrix}}_{n x n} = \\underbrace{\\begin{pmatrix} | & ... & | \\\\ u_1 & ... & u_n \\\\ | & ... & | \\\\ \\end{pmatrix}}_{n x n} \\underbrace{\\begin{pmatrix} \\sigma_1 & 0 & 0 \\\\ 0 & ... & 0 \\\\ 0 & 0 & \\sigma_n \\\\ \\end{pmatrix}}_{n x n} \\underbrace{\\begin{pmatrix} - & u_1^H & - \\\\ ... & ... & ... \\\\ - & u_n^H & - \\\\ \\end{pmatrix}}_{n x n}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U=\n",
      " [[-0.36  0.66  0.34  0.47 -0.32]\n",
      " [-0.39 -0.56 -0.11  0.07 -0.72]\n",
      " [-0.55  0.16 -0.75  0.1   0.3 ]\n",
      " [-0.52  0.12  0.33 -0.77  0.06]\n",
      " [-0.37 -0.46  0.45  0.41  0.53]] \n",
      "\n",
      "D=\n",
      " [2.58 0.68 0.58 0.11 0.01] \n",
      "\n",
      "Vh=\n",
      " [[-0.55 -0.41 -0.51 -0.41 -0.32]\n",
      " [-0.59  0.17 -0.23  0.43  0.62]\n",
      " [-0.25 -0.59  0.7  -0.16  0.29]\n",
      " [-0.08  0.51  0.08 -0.78  0.35]\n",
      " [ 0.53 -0.45 -0.44 -0.14  0.55]] \n",
      "\n",
      "M=\n",
      " [[0.19 0.37 0.51 0.5  0.65]\n",
      " [0.79 0.39 0.56 0.26 0.07]\n",
      " [0.84 0.86 0.4  0.69 0.4 ]\n",
      " [0.65 0.41 0.79 0.63 0.51]\n",
      " [0.64 0.21 0.74 0.18 0.2 ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "M = np.random.rand(5,5)\n",
    "#decomposition\n",
    "U,D,Vh = np.linalg.svd(M)\n",
    "#test by reversing back\n",
    "M = np.round( np.matmul(np.matmul(U,np.diag(D)),Vh) ,2)\n",
    "\n",
    "print(\"U=\\n\",np.round(U,2),\"\\n\")\n",
    "print(\"D=\\n\",np.round(D,2),\"\\n\")\n",
    "print(\"Vh=\\n\",np.round(Vh,2),\"\\n\")\n",
    "print(\"M=\\n\",np.round(M,2),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation calculation\n",
    "The correlation of an Matrix is defined as $corr = X^H X$ (correlation of column vectors [data samples]) or alternatively as $corr = X X^H$ (correlation of row vectors [data parameters]). It <br>\n",
    "\n",
    "\\begin{equation}\n",
    "X^H X = \\begin{pmatrix} - & x_1^H & - \\\\ ... & ... & ... \\\\ - & x_m^H & - \\\\ \\end{pmatrix} \\begin{pmatrix} | & ... & | \\\\ x_1 & ... & x_m \\\\ | & ... & | \\\\ \\end{pmatrix} = \\begin{pmatrix} x_1^H x_1 & ... & x_1^H x_m \\\\ ... & ... & ... \\\\ x_m^H x_1 & ... & x_m^H x_m \\\\ \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "X X^H = \\begin{pmatrix} | & ... & | \\\\ x_1 & ... & x_m \\\\ | & ... & | \\\\ \\end{pmatrix} \\begin{pmatrix} - & x_1^H & - \\\\ ... & ... & ... \\\\ - & x_m^H & - \\\\ \\end{pmatrix} = \\begin{pmatrix} ... & ... & ... \\\\ ... & ... & ... \\\\ ... & ... & ... \\\\ \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "This correlation matrices are symmetrical, square, have the same rank as X and have positive eigenvalues.\n",
    "They can be computed using sigular value decomposition.\n",
    "\n",
    "\\begin{equation}\n",
    "corr = X^H X = V \\Sigma U^H U \\Sigma V^H = V \\Sigma^2 V^H \\\\\n",
    "corr = X X^H = U \\Sigma V^H V \\Sigma U^H = U \\Sigma^2 U^H \\\\\n",
    "\\end{equation}\n",
    "\n",
    "This can be reformulated to an eigenvalue problem:<br>\n",
    "\n",
    "\\begin{equation}\n",
    "corr = X^H X V = V \\Sigma^2 \\\\\n",
    "corr = X X^H U = U \\Sigma^2 \\\\\n",
    "\\end{equation}\n",
    "\n",
    "From this follows, that the correlation matricies have the same eigenvalues $\\Sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## approximation with SVD (compression)\n",
    "\n",
    "By the singular value decomposition we can approximate matrix X by the greatest $\\sigma_i$ elements: \n",
    "\n",
    "\\begin{equation}\n",
    "X \\approx \\tilde{X} = \\sigma_1 u_1 v_1^H + \\sigma_2 u_2 v_2^H + ... \\\\\n",
    "= \\sigma_1 \\begin{pmatrix} | \\\\ u_1 \\\\ | \\\\ \\end{pmatrix} \\begin{pmatrix} - & v_1^H & - \\\\ \\end{pmatrix} + \\sigma_2 \\begin{pmatrix} | \\\\ u_2 \\\\ | \\\\ \\end{pmatrix} \\begin{pmatrix} - & v_2^H & - \\\\ \\end{pmatrix} + ...\n",
    "\\end{equation}\n",
    "\n",
    "According to the Eckard-Young Teorem this approximation has the smallest frobenius norm ($l_2$) compared to all possible approximations: $argmin_{\\tilde{X}} \\Big( \\sum_{i,j} |x_{ij}-\\tilde{x}_{ij}|_2 \\Big)$\n",
    "\n",
    "### solve linear system of equations with SVD\n",
    "<b>can be used for regression</b><br>\n",
    "\n",
    "A linear System of equations $Ax = b$ can only be solved unambiguously, if the matrix A is a square matrix of full rank. Otherwise there will be infetly many or no exact solutions. <br>\n",
    "We can decompose matrix A into $A = U \\Sigma V^H$. By subtituion we get $U \\Sigma V^H x = b$, wich can be rewritten to $x = V \\Sigma^{-1} U^H b$. $A^+ = V \\Sigma^{-1} U^H$ is called the Moore-Penrose Pseudoinverse. <br>\n",
    "For a underdetermined linear system (infinitely many solutions) $\\tilde{x} = A^+ b$ produces the solution with the smallest $l_2$ norm of x:\n",
    "\n",
    "\\begin{equation}\n",
    "min_x |\\tilde{x}|_2 \\quad \\text{with} \\quad A \\tilde{x} = b\n",
    "\\end{equation}\n",
    "\n",
    "For a overdetermined linear system (no exact solutions) $\\tilde{x} = A^+ b$ produces the solution with the smallest $l_2$ error from b:\n",
    "\n",
    "\\begin{equation}\n",
    "min_x |A\\tilde{x}-b|_2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal componen analysis PCA\n",
    "\n",
    "PCA is the method to apply SVD to data to transform it into another coordinate system.\n",
    "The data (apples) is arranged in a matrix, where each row represent a measurement quantaty (size,weight,color) and each column a specific value for each sample.\n",
    "Before applying SVD each row is normalised to a mean of 0.\n",
    "\n",
    "The eigenvectors of the covariance matrix are computed to get $\\Sigma$ and the eigenvectors are equal to $V$.\n",
    "The matrix $U^H$ from the SVD isn't calculated, since it's not used for PCA.\n",
    "The the Data can be approximately represented with a fraction of the biggest eigenvalues wich lie along corresponding eigenvectors. The data is now transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
